{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f68f79-92d7-41f8-86ec-5c22d25593eb",
   "metadata": {},
   "source": [
    "# **ANALYZING YOUTUBE COMMENTS ON ENVIRONMENTAL SUSTAINABILITY TRENDS**\n",
    "### A TDI Data Science Capstone Project Presented by Diane Christine Pelejo\n",
    "   \n",
    "   \n",
    "In recent news cycles, we have been seeing environmental catastrophes unfold one after\n",
    "another. This has increased awareness among everyday people, making us more conscientious\n",
    "about our actions and how we can minimize the negative impacts of our life choices on Mother\n",
    "Nature. Living an environmentally sustainable lifestyle is a goal that many of us wish to achieve.\n",
    "Admittedly, it can get overwhelming to figure out what we can realistically do to move towards\n",
    "this goal. Several sustainability trends have gained popularity such as: installing solar panel at\n",
    "home, driving electric cars, composting, using paper straws and so on and so forth. But not all\n",
    "of us can practice or pursue these trends and some may even be problematic. In this capstone\n",
    "project, I wish to explore patterns in the way people talk about different sustainability trends. \n",
    "\n",
    "I looked at comments on youtube videos related to some environmental sustainability trends and analyze the subtopics being discussed by the commenters on these videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46617e8-67ae-4301-ae06-6618dcedd7d7",
   "metadata": {},
   "source": [
    "# Collecting Data\n",
    "\n",
    "\n",
    "Here we call the python scrip `scrape_yt_comments.py` to collect information on top videos (maximum of 150 videos) related to an environmental sustainability trend. The search keywords we use are listed `keywords`.\n",
    "\n",
    "\n",
    "```python\n",
    "keywords=['solar energy', 'composting', 'paper straws', 'electric vehicles']\n",
    "\n",
    "import scrape_yt_comments\n",
    "from scrape_yt_comments import *\n",
    "\n",
    "num_vids=150\n",
    "for keyword in keywords: \n",
    "    num=0\n",
    "    while num<num_vids:\n",
    "        try:\n",
    "            print(\"Searching for videos related to:\", keyword,\"...\") \n",
    "            vids=vid_by_kw(keyword)\n",
    "            num+=len(vids)\n",
    "            for i in vids:\n",
    "                print(f\"Retrieving comments on video {i[0]} which has {i[1]} comments\" )\n",
    "                com_by_vid(i[0],int(i[1]))\n",
    "        except Exception as e:\n",
    "            print('The error raised is:', e)\n",
    "            traceback_output = traceback.format_exc()\n",
    "            print(traceback_output)   \n",
    "            break\n",
    "```\n",
    "\n",
    "The data is saved in an sqlite database file called `capstone.db`. We can look at the schema information of this database using the following code:\n",
    "\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "URL_DB = 'sqlite:///capstone.db'\n",
    "db_engine = create_engine(URL_DB)\n",
    "conn=db_engine.connect()\n",
    "\n",
    "schema=pd.read_sql_table('sqlite_master',conn)\n",
    "schema \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e6d39-672e-4998-ab7c-58e0177f0a64",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "\n",
    "We need to do some pre-processing of the data to remove some redundant rows and to create separate tables for each trend. We use the following code.\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "URL_DB = 'sqlite:///capstone.db'\n",
    "db_engine = create_engine(URL_DB)\n",
    "conn=db_engine.connect()\n",
    "\n",
    "#schema=pd.read_sql_table('sqlite_master',conn)\n",
    "\n",
    "df_vids=pd.read_sql_table('youtube_videos',conn)\n",
    "df_vids=df_vids.astype({'publishedAt': 'datetime64[ns]','num_views':int, 'num_likes':'int','num_comments':int})\n",
    "\n",
    "df_comms=pd.read_sql_table('youtube_comments',conn)\n",
    "df_comms=df_comms.astype({'published_date': 'datetime64[ns]'})\n",
    "\n",
    "#Videos 1-150  -> solar energy (146 unique videos)\n",
    "l1=list(range(150))\n",
    "solar_vids=df_vids.iloc[l1].groupby(['video_url','title','description','publishedAt']).max().reset_index()\n",
    "solar_vids.to_sql('solar_energy_videos',con=conn,if_exists='replace',index=False)\n",
    "\n",
    "solar_comments=df_comms[df_comms['video_url'].isin(solar_vids['video_url'])]\n",
    "solar_comments=solar_comments.groupby(['comment_id']).max().reset_index()\n",
    "solar_comments.to_sql('solar_energy_comments',con=conn,if_exists='replace',index=False)\n",
    "\n",
    "\n",
    "\n",
    "#151-300 #set aside 301-1844 -> composting (654 unique videos)\n",
    "l2=list(range(150,300)) #l2=list(range(150,1845)) \n",
    "compost_vids=df_vids.iloc[l2].groupby(['video_url','title','description','publishedAt']).max().reset_index()\n",
    "compost_vids.to_sql('composting_videos',con=conn,if_exists='replace',index=False)\n",
    "\n",
    "compost_comments=df_comms[df_comms['video_url'].isin(compost_vids['video_url'])]\n",
    "compost_comments=compost_comments.groupby(['comment_id']).max().reset_index()\n",
    "compost_comments.to_sql('composting_comments',con=conn,if_exists='replace',index=False)\n",
    "\n",
    "\n",
    "#1845-1994 -> paper straws (142 unique videos)\n",
    "l3=list(range(1845,1995))\n",
    "straw_vids=df_vids.iloc[l3].groupby(['video_url','title','description','publishedAt']).max().reset_index()\n",
    "straw_vids.to_sql('paper_straws_videos',con=conn,if_exists='replace',index=False)\n",
    "\n",
    "straw_comments=df_comms[df_comms['video_url'].isin(straw_vids['video_url'])]\n",
    "straw_comments=straw_comments.drop_duplicates().groupby(['comment_id']).max().reset_index()\n",
    "straw_comments.to_sql('paper_straws_comments',con=conn,if_exists='replace',index=False)\n",
    "\n",
    "#1995-2144 -> electric vehicles (147 unique videos)\n",
    "l4=list(range(1995,2144))\n",
    "EV_vids=df_vids.iloc[l4].groupby(['video_url','title','description','publishedAt']).max().reset_index()\n",
    "EV_vids.to_sql('electric_vehicles_videos',con=conn,if_exists='replace',index=False)\n",
    "\n",
    "EV_comments=df_comms[df_comms['video_url'].isin(EV_vids['video_url'])]\n",
    "EV_comments=EV_comments.drop_duplicates().groupby(['comment_id']).max().reset_index()\n",
    "EV_comments.to_sql('electric_vehicles_comments',con=conn,if_exists='replace',index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a785d8-72f1-499a-909d-f88c1560b4ec",
   "metadata": {},
   "source": [
    "# Loading the Data For Analysis\n",
    "\n",
    "Now we are ready to do our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "de3ffd9b-2f38-4feb-9e0d-5c9379bcab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We collected 146 videos and 183530 related to \"solar energy\".\n",
      "We collected 137 videos and 47133 related to \"composting\".\n",
      "We collected 142 videos and 30619 related to \"paper straws\".\n",
      "We collected 147 videos and 259811 related to \"electric vehicles\".\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "URL_DB = 'sqlite:///capstone.db'\n",
    "db_engine = create_engine(URL_DB)\n",
    "conn=db_engine.connect()\n",
    "\n",
    "solar_vids=pd.read_sql_table('solar_energy_videos',conn)\n",
    "solar_comms=pd.read_sql_table('solar_energy_comments',conn)\n",
    "print(f'We collected {len(solar_vids)} videos and {len(solar_comms)} related to \"solar energy\".') \n",
    "\n",
    "compost_vids=pd.read_sql_table('composting_videos',conn)\n",
    "compost_comms=pd.read_sql_table('composting_comments',conn)\n",
    "print(f'We collected {len(compost_vids)} videos and {len(compost_comms)} related to \"composting\".') \n",
    "\n",
    "straw_vids=pd.read_sql_table('paper_straws_videos',conn)\n",
    "straw_comms=pd.read_sql_table('paper_straws_comments',conn)\n",
    "print(f'We collected {len(straw_vids)} videos and {len(straw_comms)} related to \"paper straws\".') \n",
    "\n",
    "EV_vids=pd.read_sql_table('electric_vehicles_videos',conn)\n",
    "EV_comms=pd.read_sql_table('electric_vehicles_comments',conn)\n",
    "print(f'We collected {len(EV_vids)} videos and {len(EV_comms)} related to \"electric vehicles\".') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d817b3f-ca4f-4317-aee0-7cc534e976c9",
   "metadata": {},
   "source": [
    "# Time-Series Analysis of Video Metrics (Comments, Likes, Views)\n",
    "\n",
    "First, for each trend we will plot the number of videos by month of publication and the number of comments by month of posting and see the trend on how many comments a trend gets each month on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4362acf0-ad3c-4418-885b-9e2fa7a620ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "def plot_monthly(trend, vids_df, comms_df):\n",
    "    vid_count=(vids_df['publishedAt']\n",
    "    .dt.to_period('M')\n",
    "    .dt.to_timestamp()\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'month','publishedAt':'num_videos'})\n",
    "    )\n",
    "\n",
    "    comm_count=(comms_df['published_date']\n",
    "    .dt.to_period('M')\n",
    "    .dt.to_timestamp()\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'month','published_date':'num_comments'})\n",
    "    )\n",
    "\n",
    "    summary=vid_count.merge(comm_count, on='month',how='outer').fillna(0)\n",
    "    summary=summary.set_index('month')\n",
    "    summary=summary.sort_index()\n",
    "    summary['num_vids_pub']=summary['num_videos'].cumsum()\n",
    "    summary['norm_comm']=summary['num_comments']/summary['num_vids_pub']\n",
    "\n",
    "    fig, (ax1,ax2,ax3) = plt.subplots(nrows=3, sharex=True)\n",
    "    fig.suptitle(trend, fontsize=16, fontweight='bold',y=0.92)\n",
    "    fig.set_figheight(6.9)\n",
    "    fig.set_figwidth(5)\n",
    "    fig.align_ylabels()\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(hspace=.1)# remove vertical gap between subplots\n",
    "    plt.xlabel('date published',fontsize=12, fontstyle='italic',fontweight='bold',labelpad=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    line1, = ax1.plot(summary['norm_comm'], color='b', marker='.', markersize=3)\n",
    "    line2, =  ax2.plot(summary['num_comments'], color='r', marker='.', markersize=3)\n",
    "    line3, =  ax3.plot(summary['num_videos'], color='g', marker='.', markersize=3)\n",
    "    line4, = ax3.plot(summary['num_vids_pub'],color='orange',marker='.', markersize=3)\n",
    "    \n",
    "    ax1.set_ylabel(r'$\\bf\\frac{no.\\ new \\ comments}{no.\\ existing\\ videos}$',fontsize=12)\n",
    "    ax2.set_ylabel('no. new comments',fontsize=10, fontstyle='italic',fontweight='bold')\n",
    "    ax3.set_ylabel('no. videos',fontsize=10, fontstyle='italic',fontweight='bold')\n",
    "    ax3.xaxis.set_major_locator(matplotlib.dates.YearLocator(base=1))\n",
    "    ax3.legend((line3,line4),('newly published','total so far'),loc='upper left')\n",
    "    \n",
    "    plt.savefig(trend.replace(\" \", \"_\")+'.jpg',bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "plot_monthly('solar energy', solar_vids, solar_comms)\n",
    "plot_monthly('composting', compost_vids, compost_comms)\n",
    "plot_monthly('paper straws', straw_vids, straw_comms)\n",
    "plot_monthly('electric vehicles', EV_vids, EV_comms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c3564-8e73-4159-8f79-2f2d95ecbd6c",
   "metadata": {},
   "source": [
    "### On Publication Dates of Top Videos Collected and their comments\n",
    "\n",
    "| ![](solar_energy.jpg) | ![](composting.jpg) |\n",
    "|-|-|\n",
    "| ![](paper_straws.jpg) | ![](electric_vehicles.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d32258-13ae-4a95-8341-909c4c29ac2a",
   "metadata": {},
   "source": [
    "Next, we have some information on the number of likes, views and comments of videos but we don't have a timestamp for when the likes and views were acquired. Here, we look at the individual videos and the number of likes, views and comments they have as of data collection. We arrange the videos by publication date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7db3c80f-05d4-4785-b357-cb4f4e6d7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend='solar energy'\n",
    "vids_df=solar_vids\n",
    "comms_df=solar_comms\n",
    "\n",
    "def metrics_trend(trend, vids_df):\n",
    "\n",
    "    temp=vids_df[['publishedAt','num_views','num_likes','num_comments']].sort_values('publishedAt',ignore_index=True).set_index('publishedAt')\n",
    "    temp['views_cum_avg']=temp['num_views'].expanding().mean()\n",
    "    temp['views_cum_std']=temp['num_views'].expanding().std(ddof=0)\n",
    "    temp['likes_cum_avg']=temp['num_likes'].expanding().mean()\n",
    "    temp['likes_cum_std']=temp['num_likes'].expanding().std(ddof=0)\n",
    "    temp['comms_cum_avg']=temp['num_comments'].expanding().mean()\n",
    "    temp['comms_cum_std']=temp['num_comments'].expanding().std(ddof=0)\n",
    "\n",
    "    fig, (ax1,ax2,ax3) = plt.subplots(nrows=3, sharex=True)\n",
    "\n",
    "    fig.suptitle(trend, fontsize=16, fontweight='bold',y=0.92)\n",
    "    fig.set_figheight(6.9)\n",
    "    fig.set_figwidth(5)\n",
    "    fig.align_ylabels()\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(hspace=.1)# remove vertical gap between subplots\n",
    "    plt.xlabel('video index (publication date-time)',fontsize=12, fontstyle='italic',fontweight='bold',labelpad=12)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    line11, = ax1.plot(temp['num_views'], color='b', marker='.', markersize=3)\n",
    "    line12, =ax1.plot(temp['num_views'].expanding().mean(),color='orange',marker='.', markersize=3)\n",
    "    ax1.fill_between(temp.index,temp['views_cum_avg'] - temp['views_cum_std'], temp['views_cum_avg'] + temp['views_cum_std'], color='orange', alpha=0.2)\n",
    "    ax1.legend((line11,line12),('individual video','cum. avg.\\n (1 std range)'),loc='upper left',fontsize=8)\n",
    "    ax1.set_ylabel('no. views',fontsize=12, fontstyle='italic',fontweight='bold')\n",
    "    ax1.ticklabel_format(style='sci', scilimits=(0,0), axis='y', useOffset=True, useLocale=None, useMathText=True)\n",
    "\n",
    "    line21, =  ax2.plot(temp['num_likes'], color='r', marker='.', markersize=3)\n",
    "    line22, =ax2.plot(temp['num_likes'].expanding().mean(),color='orange',marker='.', markersize=3)\n",
    "    ax2.fill_between(temp.index,temp['likes_cum_avg'] - temp['likes_cum_std'], temp['likes_cum_avg'] + temp['likes_cum_std'], color='orange', alpha=0.2)\n",
    "    ax2.legend((line11,line12),('individual video','cum. avg. \\n(1 std range)'),loc='upper left',fontsize=8)\n",
    "    ax2.set_ylabel('no. likes',fontsize=12, fontstyle='italic',fontweight='bold')\n",
    "    ax2.ticklabel_format(axis='y', style='sci', scilimits=(0,0), useOffset=True, useLocale=None, useMathText=True)\n",
    "\n",
    "\n",
    "\n",
    "    ax3.legend((line11,line12),('individual video','cum. avg. (1 std range)'),loc='upper left',fontsize=8)\n",
    "    line31, =  ax3.plot(temp['num_comments'], color='g', marker='.', markersize=3)\n",
    "    line32, =ax3.plot(temp['num_comments'].expanding().mean(),color='orange',marker='.', markersize=3)\n",
    "    ax3.fill_between(temp.index,temp['comms_cum_avg'] - temp['comms_cum_std'], temp['comms_cum_avg'] + temp['comms_cum_std'], color='orange', alpha=0.2)\n",
    "    ax3.set_ylabel('no. comments',fontsize=12, fontstyle='italic',fontweight='bold')\n",
    "    ax3.ticklabel_format(axis='y', style='sci', scilimits=(0,0), useOffset=True, useLocale=None, useMathText=True)\n",
    "    ax3.xaxis.set_major_locator(matplotlib.dates.YearLocator(base=1))\n",
    "    \n",
    "    plt.savefig(trend.replace(\" \", \"_\")+'_metrics.jpg',bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "metrics_trend('solar energy', solar_vids)\n",
    "metrics_trend('composting', compost_vids)\n",
    "metrics_trend('paper straws', straw_vids)\n",
    "metrics_trend('electric vehicles', EV_vids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0901c7-f132-49c4-8566-66d068dfae9c",
   "metadata": {},
   "source": [
    "| ![](solar_energy_metrics.jpg) | ![](composting_metrics.jpg) |\n",
    "|-|-|\n",
    "| ![](paper_straws_metrics.jpg) | ![](electric_vehicles_metrics.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c232832-3166-464e-8150-d69f89be1d0a",
   "metadata": {},
   "source": [
    "Finally, we look at the trend on the number of comments a video gets days its publication date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3648df70-02e7-48a7-bbd7-6004334899f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend='solar energy'\n",
    "vids_df=solar_vids\n",
    "comms_df=solar_comms\n",
    "\n",
    "def days_after_vidpub(trend,vids_df,comms_df):\n",
    "    new_df=comms_df.merge(vids_df[['video_url','publishedAt']], on='video_url').rename(columns={'publishedAt':'video_pub_date'})\n",
    "    new_df['days_vid_to_comment']=(new_df['published_date'].dt.date-new_df['video_pub_date'].dt.date).dt.days\n",
    "    temp3=new_df.groupby(['video_url','days_vid_to_comment'])['comments'].count().reset_index()\n",
    "    grid=sns.lineplot(data=temp3, x='days_vid_to_comment', y='comments')\n",
    "    grid.set(xscale='log',yscale='log')\n",
    "    plt.title(trend,fontsize=16, fontweight='bold',y=0.92)\n",
    "    plt.xlabel('no. of days after video publication',fontsize=10, fontweight='bold', fontstyle='italic',)\n",
    "    plt.ylabel('avg. no. of new comments',fontsize=10, fontweight='bold', fontstyle='italic',)\n",
    "    plt.savefig(trend.replace(\" \", \"_\")+'_days.jpg',bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "days_after_vidpub('solar energy', solar_vids, solar_comms)\n",
    "days_after_vidpub('composting', compost_vids, compost_comms)\n",
    "days_after_vidpub('paper straws', straw_vids, straw_comms)\n",
    "days_after_vidpub('electric vehicles', EV_vids, EV_comms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac785d56-166a-4ab1-bbd3-eda065159ef6",
   "metadata": {},
   "source": [
    "| ![](solar_energy_days.jpg) | ![](composting_days.jpg) |\n",
    "|-|-|\n",
    "| ![](paper_straws_days.jpg) | ![](electric_vehicles_days.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4dc2e-3d9b-448e-aecc-fe12d804ffb7",
   "metadata": {},
   "source": [
    "# <center> **Topic Modeling** </center>\n",
    "\n",
    "Finally, we look at subtopics that come up in the comments for a particular trend. We use four different models to get the top 5 subtopics for each one.\n",
    "1. Non-Negative Matrix Factorization model (NMF) with Frobenius loss function, \n",
    "2. NMF with Kullback-Leibler loss function, \n",
    "3. MiniBatch NMF with Frobenius loss function; and \n",
    "4. Latent Dirichlet Allocation (LDA)) model\n",
    "\n",
    "First we do some preprocessing of the corpus of comments for each trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "57045b44-9a8c-430b-9137-5869bcddd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger') #run this once\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "my_words={'mine','myself','your','yours','yourself','yourselves', 'ours','ourselves','itself','himself','hers','herself', 'they','them','their','theirs',\n",
    "           'themselves','youre','youve','youll','youd','this','that','itll','theyll','been''isnt','arent','hasnt','have','havent','hadnt','wasnt','were','werent',\n",
    "           'dont','does','doesnt','didnt','must','mustnt','cant','could','couldt','should','shouldnt','would','wouldnt','wont', 'shant','aint','onto','also','than','just','only'}\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def process_stp(stpwrds):\n",
    "    def process_txt(txt):\n",
    "        #this block removes punctuations and escape characters from the text\n",
    "        char_dict={ord(char):' ' for char in string.punctuation}\n",
    "        char_dict.update({num:' ' for num in list(range(32))})\n",
    "        ans=txt.translate(char_dict) \n",
    "\n",
    "        #this block removes short words (length up to three)\n",
    "        shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "        ans=shortword.sub('',ans)\n",
    "        \n",
    "        #this block removes stop words from the text and lemmatizes the remaining words\n",
    "        wnl=WordNetLemmatizer()\n",
    "        return ' '.join([wnl.lemmatize(word,get_wordnet_pos(word)) for word in word_tokenize(ans) if not word in stpwrds])\n",
    "    \n",
    "    return process_txt\n",
    "\n",
    "def get_corpus(trend,comms_df):\n",
    "    search_words=set(trend.split())\n",
    "    return  comms_df['comments'].apply(process_stp(my_words.union(search_words)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, MiniBatchNMF, LatentDirichletAllocation\n",
    "\n",
    "def plot_topic_words(model,feature_names,title,file_name):\n",
    "    n_top_words=10\n",
    "    fig,axes=plt.subplots(1,5, figsize=(30,15),sharex=True)\n",
    "    axes=axes.flatten()\n",
    "    for topic_idx,topic in enumerate(model.components_):\n",
    "        top_features_ind=topic.argsort()[: -n_top_words-1:-1]\n",
    "        top_features=[feature_names[i] for i in top_features_ind]\n",
    "        weights=topic[top_features_ind]\n",
    "        ax=axes[topic_idx]\n",
    "        ax.barh(top_features,weights)\n",
    "        ax.set_title(f\"Topic {topic_idx+1}\", fontdict={\"fontsize\":30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\",which=\"major\",labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.5,wspace=0.9, hspace=0.3)\n",
    "    plt.savefig(file_name+'.jpg',bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "0ee4f37f-b17e-43a6-8529-3e9fc7feb17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_corpus=get_corpus('solar energy',solar_comms)\n",
    "compost_corpus=get_corpus('composting',compost_comms)\n",
    "straw_corpus=get_corpus('paper straws', straw_comms)\n",
    "EV_corpus=get_corpus('electric vehicles',EV_comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "4ef868ec-b10e-4073-abbc-d3e695bfdbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_corpus.to_pickle('./solar_corpus.pkl')\n",
    "compost_corpus.to_pickle('./compost_corpus.pkl')\n",
    "straw_corpus.to_pickle('./straw_corpus.pkl')\n",
    "EV_corpus.to_pickle('./EV_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "ebbbb240-ef48-4259-ac45-fa7ea9ef1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_corpus=pd.read_pickle('./solar_corpus.pkl')\n",
    "compost_corpus=pd.read_pickle('./compost_corpus.pkl')\n",
    "straw_corpus=pd.read_pickle('./straw_corpus.pkl')\n",
    "EV_corpus=pd.read_pickle('./EV_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "69250225-d26e-4639-aa37-d6b61618e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without using further stopwords on the vectorizers \n",
    "\n",
    "n_features=500\n",
    "n_components=5\n",
    "batch_size=1000\n",
    "init=\"nndsvda\"\n",
    "\n",
    "for trend,corpus in {'solar energy': solar_corpus, 'composting': compost_corpus, 'paper straws': straw_corpus, 'electric vehicles': EV_corpus}.items():\n",
    "    \n",
    "    #NMF model using frobenius norm\n",
    "    vectorizer_tfidf=TfidfVectorizer(max_df=0.9,min_df=5,max_features=n_features)\n",
    "    corpus_tfidf=vectorizer_tfidf.fit_transform(corpus)\n",
    "    features_tfidf=vectorizer_tfidf.get_feature_names_out()\n",
    "    \n",
    "    nmf1=NMF(n_components=n_components,random_state=1,init=init,beta_loss=\"frobenius\",alpha_W=0.00005,alpha_H=0.00005,l1_ratio=1).fit(corpus_tfidf)\n",
    "    plot_topic_words(nmf1,features_tfidf, f\"Topic modeling for {trend}-related Youtube video comments Using Non-Negative Matrix Factorization (NMF) with Frobenius loss function\",'ns_'+trend.replace(' ','_')+'_nmf_fro')\n",
    "    \n",
    "    #NMF model using Kullback-leibler loss function\n",
    "    nmf2 = NMF(n_components=n_components,random_state=1,init=init,beta_loss=\"kullback-leibler\",solver=\"mu\", max_iter=1000,alpha_W=0.00005,alpha_H=0.00005,l1_ratio=0.5,).fit(corpus_tfidf)\n",
    "    plot_topic_words(nmf2,features_tfidf, f\"Topic modeling for {trend}-related Youtube video comments Using NMF with Kullback-Leibler loss function\",'ns_'+trend.replace(' ','_')+'_nmf_kl')\n",
    "    \n",
    "    #MiniBatch\n",
    "    mbnmf = MiniBatchNMF(n_components=n_components,random_state=1,batch_size=batch_size, init=init,beta_loss=\"frobenius\", alpha_W=0.00005,alpha_H=0.00005,l1_ratio=0.5,).fit(corpus_tfidf)\n",
    "    plot_topic_words(mbnmf,features_tfidf, f\"Topic modeling for {trend}-related Youtube video comments Using MiniBatch NMF with Frobenius loss function\",'ns_'+trend.replace(' ','_')+'_mb_fro')\n",
    "    \n",
    "    #LDA\n",
    "    vectorizer_cvec=CountVectorizer(max_df=0.95,min_df=2,max_features=15000)\n",
    "    corpus_cvec=vectorizer_cvec.fit_transform(corpus)\n",
    "    features_cvec=vectorizer_cvec.get_feature_names_out()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_components,max_iter=5,learning_method=\"online\",learning_offset=50.0,random_state=0)\n",
    "    lda.fit(corpus_cvec)\n",
    "    plot_topic_words(lda,features_cvec, f\"Topic modeling for {trend}-related Youtube video comments Using Latent Dirichlet Allocation (LDA) NMF with Frobenius loss function\",'ns_'+trend.replace(' ','_')+'_lda')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "4fd3cf45-3ffa-4c98-a886-fc4e6b8d0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using  stopwords on the vectorizers \n",
    "\n",
    "n_features=500\n",
    "n_components=5\n",
    "batch_size=1000\n",
    "init=\"nndsvda\"\n",
    "\n",
    "for trend,corpus in {'solar energy': solar_corpus, 'composting': compost_corpus, 'paper straws': straw_corpus, 'electric vehicles': EV_corpus}.items():\n",
    "    \n",
    "    #NMF model using frobenius norm\n",
    "    vectorizer_tfidf=TfidfVectorizer(max_df=0.9,min_df=5,max_features=n_features,stop_words=\"english\")\n",
    "    corpus_tfidf=vectorizer_tfidf.fit_transform(corpus)\n",
    "    features_tfidf=vectorizer_tfidf.get_feature_names_out()\n",
    "    \n",
    "    nmf1=NMF(n_components=n_components,random_state=1,init=init,beta_loss=\"frobenius\",alpha_W=0.00005,alpha_H=0.00005,l1_ratio=1).fit(corpus_tfidf)\n",
    "    plot_topic_words(nmf1,features_tfidf, f\"Topic modeling for {trend}-related Youtube video comments Using Non-Negative Matrix Factorization (NMF) with Frobenius loss function\",trend.replace(' ','_')+'_nmf_fro')\n",
    "    \n",
    "    #NMF model using Kullback-leibler loss function\n",
    "    nmf2 = NMF(n_components=n_components,random_state=1,init=init,beta_loss=\"kullback-leibler\",solver=\"mu\", max_iter=1000,alpha_W=0.00005,alpha_H=0.00005,l1_ratio=0.5,).fit(corpus_tfidf)\n",
    "    plot_topic_words(nmf2,features_tfidf, f\"Topic modeling for {trend}-related Youtube video comments Using NMF with Kullback-Leibler loss function\",trend.replace(' ','_')+'_nmf_kl')\n",
    "    \n",
    "    #MiniBatch\n",
    "    mbnmf = MiniBatchNMF(n_components=n_components,random_state=1,batch_size=batch_size, init=init,beta_loss=\"frobenius\", alpha_W=0.00005,alpha_H=0.00005,l1_ratio=0.5,).fit(corpus_tfidf)\n",
    "    plot_topic_words(mbnmf,features_tfidf, f\"Topic modeling for {trend}-related Youtube video comments Using MiniBatch NMF with Frobenius loss function\",trend.replace(' ','_')+'_mb_fro')\n",
    "    \n",
    "    #LDA\n",
    "    vectorizer_cvec=CountVectorizer(max_df=0.95,min_df=2,max_features=15000,stop_words=\"english\")\n",
    "    corpus_cvec=vectorizer_cvec.fit_transform(corpus)\n",
    "    features_cvec=vectorizer_cvec.get_feature_names_out()\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_components,max_iter=5,learning_method=\"online\",learning_offset=50.0,random_state=0)\n",
    "    lda.fit(corpus_cvec)\n",
    "    plot_topic_words(lda,features_cvec, f\"Topic modeling for {trend}-related Youtube video comments Using Latent Dirichlet Allocation (LDA) NMF with Frobenius loss function\",trend.replace(' ','_')+'_lda')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239ce9-6b42-4eaf-a3eb-3d93656cb847",
   "metadata": {},
   "source": [
    "# <center> **Topic Modeling for Solar Energy** </center>\n",
    "\n",
    "1. Here we removed certain words from the corpus but did not remove additional stop_words while vectorizing\n",
    "![](ns_solar_energy_nmf_fro.jpg) \n",
    "![](ns_solar_energy_nmf_kl.jpg)\n",
    "![](ns_solar_energy_mb_fro.jpg)\n",
    "![](ns_solar_energy_lda.jpg)\n",
    "2. Here we used additional stop_words while vectorizing. \n",
    "![](solar_energy_nmf_fro.jpg) \n",
    "![](solar_energy_nmf_kl.jpg)\n",
    "![](solar_energy_mb_fro.jpg)\n",
    "![](solar_energy_lda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a25f8e-bdb4-4cfd-9e10-ac19fc979eee",
   "metadata": {},
   "source": [
    "# <center> **Topic Modeling for Composting** </center>\n",
    "\n",
    "1. Here we removed certain words from the corpus but did not remove additional stop_words while vectorizing\n",
    "\n",
    "![](ns_composting_nmf_fro.jpg)  \n",
    "![](ns_composting_nmf_kl.jpg) \n",
    "![](ns_composting_mb_fro.jpg)\n",
    "![](ns_composting_lda.jpg)\n",
    "\n",
    "2. Here we used additional stop_words while vectorizing. \n",
    "\n",
    "![](composting_nmf_fro.jpg)  \n",
    "![](composting_nmf_kl.jpg) \n",
    "![](composting_mb_fro.jpg)\n",
    "![](composting_lda.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048cbec-190c-4e5f-a2ea-10809235b888",
   "metadata": {},
   "source": [
    "# <center> **Topic Modeling for Paper Straws** </center>\n",
    "\n",
    "1. Here we removed certain words from the corpus but did not remove additional stop_words while vectorizing\n",
    "\n",
    "![](ns_paper_straws_nmf_fro.jpg)  \n",
    "![](ns_paper_straws_nmf_kl.jpg)\n",
    "![](ns_paper_straws_mb_fro.jpg)\n",
    "![](ns_paper_straws_lda.jpg)\n",
    "\n",
    "2. Here we used additional stop_words while vectorizing. \n",
    "\n",
    "![](paper_straws_nmf_fro.jpg)  \n",
    "![](paper_straws_nmf_kl.jpg)\n",
    "![](paper_straws_mb_fro.jpg)\n",
    "![](paper_straws_lda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03baaba1-3b7a-46b0-bd69-edd6fdd96193",
   "metadata": {},
   "source": [
    "# <center> **Topic Modeling for Electric Vehicles** </center>\n",
    "\n",
    "1. Here we removed certain words from the corpus but did not remove additional stop_words while vectorizing\n",
    "\n",
    "![](paper_straws_nmf_fro.jpg)  \n",
    "![](paper_straws_nmf_kl.jpg)\n",
    "![](paper_straws_mb_fro.jpg)\n",
    "![](paper_straws_lda.jpg)\n",
    "\n",
    "2. Here we used additional stop_words while vectorizing. \n",
    "\n",
    "![](paper_straws_nmf_fro.jpg)  \n",
    "![](paper_straws_nmf_kl.jpg)\n",
    "![](paper_straws_mb_fro.jpg)\n",
    "![](paper_straws_lda.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDI capstone",
   "language": "python",
   "name": "tdi-cs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
